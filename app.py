
#. streamlit run app.py
import streamlit as st
from llama_cpp import Llama

# Configuration de la page
st.set_page_config(
    page_title="Assistant M√©dical",
    page_icon="üè•",
    layout="wide"
)

# System prompt
SYSTEM_PROMPT = """Tu es un assistant m√©dical grand public destin√© √† fournir uniquement de l'information
m√©dicale g√©n√©rale et √©ducative, √† destination du grand public.

P√âRIM√àTRE AUTORIS√â :
- Expliquer des sympt√¥mes courants de mani√®re g√©n√©rale
- Donner des informations de sant√© fiables, non personnalis√©es
- D√©crire quand il est habituel de consulter un professionnel de sant√©
- Donner des conseils g√©n√©raux de pr√©vention et d'hygi√®ne de vie
- Orienter vers un m√©decin ou une t√©l√©consultation lorsque n√©cessaire

INTERDICTIONS STRICTES :
- Ne jamais poser de diagnostic
- Ne jamais confirmer ou infirmer une maladie
- Ne jamais prescrire de traitement ou de m√©dicament
- Ne jamais interpr√©ter des examens m√©dicaux ou r√©sultats d'analyse
- Ne jamais donner d'avis m√©dical personnalis√©
- Ne jamais minimiser un risque grave

STYLE DE R√âPONSE :
- Ton clair, bienveillant, rassurant et non alarmiste
- Langage simple et accessible
- Toujours rappeler que l'information fournie ne remplace pas un avis m√©dical
- Structurer les r√©ponses (paragraphes courts ou listes)"""

@st.cache_resource
def load_model():
    """Charge le mod√®le Mistral 7B GGUF"""
    try:
        llm = Llama(
            model_path="src/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",  # Chemin √† adapter
            n_ctx=4096,  # Taille du contexte
            n_threads=8,  # Nombre de threads CPU
            n_gpu_layers=0,  # Mettre > 0 si GPU disponible
            verbose=False
        )
        return llm
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le : {e}")
        return None

def format_prompt(messages):
    """Formate les messages pour Mistral Instruct"""
    formatted = f"<s>[INST] {SYSTEM_PROMPT}\n\n"
    
    for i, msg in enumerate(messages):
        if msg["role"] == "user":
            if i == 0:
                formatted += f"{msg['content']} [/INST]"
            else:
                formatted += f"[INST] {msg['content']} [/INST]"
        elif msg["role"] == "assistant":
            formatted += f" {msg['content']}</s>"
    
    return formatted

def generate_response(llm, messages):
    """G√©n√®re une r√©ponse avec le mod√®le"""
    prompt = format_prompt(messages)
    
    response = llm(
        prompt,
        max_tokens=1024,
        temperature=0.7,
        top_p=0.95,
        repeat_penalty=1.1,
        stop=["</s>", "[INST]"],
        echo=False
    )
    
    return response['choices'][0]['text'].strip()

def main():
    st.title("üè• Assistant M√©dical Grand Public")
    st.caption("‚ö†Ô∏è Cet assistant fournit des informations g√©n√©rales uniquement. Consultez un professionnel de sant√© pour un avis m√©dical personnalis√©.")
    
    # Initialisation de l'historique dans session_state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Bouton pour rafra√Æchir la conversation
    col1, col2 = st.columns([6, 1])
    with col2:
        if st.button("üîÑ Nouvelle conversation", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    # Chargement du mod√®le
    llm = load_model()
    
    if llm is None:
        st.error("Impossible de charger le mod√®le. V√©rifiez le chemin du fichier GGUF.")
        st.stop()
    
    # Affichage de l'historique des messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input utilisateur
    if prompt := st.chat_input("Posez votre question m√©dicale..."):
        # Ajouter le message utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G√©n√©rer la r√©ponse
        with st.chat_message("assistant"):
            with st.spinner("G√©n√©ration de la r√©ponse..."):
                try:
                    response = generate_response(llm, st.session_state.messages)
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    st.error(f"Erreur lors de la g√©n√©ration : {e}")

    # Sidebar avec informations
    with st.sidebar:
        st.header("‚ÑπÔ∏è Informations")
        st.info("""
        **Cet assistant peut :**
        - Expliquer des sympt√¥mes courants
        - Donner des informations sant√© g√©n√©rales
        - Conseiller quand consulter
        
        **Il ne peut pas :**
        - Poser de diagnostic
        - Prescrire des m√©dicaments
        - Interpr√©ter des examens
        """)
        
        st.header("üìä Param√®tres")
        st.write(f"Messages dans l'historique : {len(st.session_state.messages)}")
        
        if st.button("‚ö†Ô∏è Effacer l'historique"):
            st.session_state.messages = []
            st.rerun()

if __name__ == "__main__":
    main()