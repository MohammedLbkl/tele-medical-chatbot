{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db32077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "REPO_PATH = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "    \n",
    "from llama_cpp import Llama\n",
    "from src.prompts.prompts import SYSTEM_PROMPT, SYSTEM_PROMPT_CLASSIFICATION\n",
    "from src.app.llm import generate_response\n",
    "from src.app.classif import generate_response_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8abeb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "MODELS_CONFIG = {\n",
    "    \"mistral-7b\": { # 4.3Go\n",
    "        \"path\": \"../models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\", # C'est la version conçue pour le chat et les instructions\n",
    "    },\n",
    "    \"qwen2.5-1.5b\": { # 1.6Go\n",
    "        \"path\": \"../models/qwen2.5-1.5b-instruct-q8_0.gguf\", # Excellente performance, en particulier sur les langues non-anglaises.\n",
    "    },\n",
    "    \"qwen2.5-7b\": { # 1.6Go\n",
    "        \"path\": \"../models/qwen2.5-7b-instruct-q2_k.gguf\", # Excellente performance, en particulier sur les langues non-anglaises.\n",
    "    },\n",
    "    \"qwen3-2b\": { # 1.6Go\n",
    "        \"path\": \"../models/Qwen3-1.7B-Q8_0.gguf\", # Excellente performance, en particulier sur les langues non-anglaises.\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_PATH = MODELS_CONFIG[\"mistral-7b\"][\"path\"]\n",
    "MODEL_PATH = MODELS_CONFIG[\"qwen2.5-1.5b\"][\"path\"]\n",
    "MODEL_PATH = MODELS_CONFIG[\"qwen2.5-7b\"][\"path\"]\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=2048, \n",
    "    chat_format=\"chatml\",\n",
    "    verbose=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23af0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "    \"\"\"\n",
    "    Fonction qui envoie la phrase de l'utilisateur au LLM et traite le résultat.\n",
    "    Utilise une approche \"few-shot\" avec 4 exemples diversifiés pour guider le modèle.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "def generate_response_classification(question):\n",
    "    \"\"\"\n",
    "    Fonction qui envoie la phrase de l'utilisateur au LLM et traite le résultat.\n",
    "    Utilise une approche \"few-shot\" avec 4 exemples diversifiés pour guider le modèle.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CLASSIFICATION},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94efc5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : J’ai mal à la gorge depuis deux jours, est-ce grave ?\n",
      "Réponse : Il est normal de ressentir une douleur à la gorge de temps en temps. Cela peut être causé par de simples irritations de la muqueuse de la gorge, souvent dues à la déshydratation, la fume ou l'exposition à des irritants comme la fumée ou le pollen.\n",
      "\n",
      "Cependant, si votre gorge est très douloureuse, si vous avez des difficultés à manger ou à boire, ou si vous avez des pharyngites associées tels que des glandes sous la langue ou dans le cou qui sont gonflées, il est important de consulter un professionnel de la santé.\n",
      "\n",
      "En général, ces symptômes peuvent être dus à des infections virales ou bactériennes, mais il est important de noter que seuls un professionnel de la santé peut diagnostiquer et traiter les infections.\n",
      "\n",
      "Rappelez-vous, il est toujours préférable de consulter un médecin si vous avez des inquiétudes ou si vos symptômes persistent ou s'aggravent.\n",
      "Proba : 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_questions_grave = [\"J’ai mal à la gorge depuis deux jours, est-ce grave ?\"]\n",
    "    \n",
    "for question in list_questions_grave:\n",
    "    reponse = generate_response(question)\n",
    "    proba = generate_response_classification(question)\n",
    "    print(f\"Question : {question}\\nRéponse : {reponse}\\nProba : {proba}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450a4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
